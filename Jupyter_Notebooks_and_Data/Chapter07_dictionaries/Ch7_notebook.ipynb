{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Sample Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all code from Chapter 7: _Dictionary-Based Textual Analysis_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Identifying Words and Sentences in Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'invested', 'in', 'six', 'areas', 'of', 'the', 'business', 'that', 'account', 'for', 'nearly', 'of', 'total', \"Macy's\", 'sales', 'Dresses', 'fine', 'jewelry', 'big', 'ticket', \"men's\", 'tailored', \"women's\", 'shoes', 'and', 'beauty', 'these', 'investments', 'were', 'aimed', 'at', 'driving', 'growth', 'through', 'great', 'products', 'top-performing', 'colleagues', 'improved', 'environment', 'and', 'enhanced', 'marketing', 'All', 'six', 'areas', 'continued', 'to', 'outperform', 'the', 'balance', 'of', 'the', 'business', 'on', 'market', 'share', 'return', 'on', 'investment', 'and', 'profitability', 'And', 'we', 'capture', 'approximately', 'of', 'the', 'market', 'in', 'these', 'categories']\n",
      "73\n"
     ]
    }
   ],
   "source": [
    "# input text\n",
    "text = \"\"\"We invested in six areas of the business that account for nearly 40% of total Macy's sales. \n",
    "Dresses, fine jewelry, big ticket, men's tailored, women's shoes and beauty, these investments were aimed \n",
    "at driving growth through great products, top-performing colleagues, improved environment and enhanced \n",
    "marketing. All six areas continued to outperform the balance of the business on market share, return on \n",
    "investment and profitability. And we capture approximately 9% of the market in these categories.\"\"\"\n",
    "\n",
    "# Regex \"\\b[a-zA-Z\\'\\-]+\\b\" searches for all words in text, allowing apostrophes and hyphens in words, \n",
    "# e.g., company's, state-of-the-art\n",
    "x = re.findall(r\"\\b[a-zA-Z\\'\\-]+\\b\", text)\n",
    "\n",
    "print(x)\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 We invested in six areas of the business that account for nearly 40% of total Macy's sales.\n",
      "\n",
      "2 Dresses, fine jewelry, big ticket, men's tailored, women's shoes and beauty, these investments were aimed \n",
      "at driving growth through great products, top-performing colleagues, improved environment and enhanced \n",
      "marketing.\n",
      "\n",
      "3 All six areas continued to outperform the balance of the business on market share, return on \n",
      "investment and profitability.\n",
      "\n",
      "4 And we capture approximately 9% of the market in these categories.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Regex pattern that identifies a sentence\n",
    "# re.compile compile a regular expression pattern into a regular expression object in Python\n",
    "sentence_regex = re.compile(r\"\\b[A-Z](?:[^\\.!?]|\\.\\d)*[\\.!?]\") \n",
    "\n",
    "# function that identifies sentences\n",
    "def identify_sentences (input_text:str): \n",
    "    # finds all matches of sentence_regex in input_text\n",
    "    sentences = re.findall(sentence_regex, input_text) \n",
    "    return sentences\n",
    "\n",
    "# applies identify_sentences function on text in the previous example\n",
    "sentences = identify_sentences(text) \n",
    "\n",
    "# enumerate is a Python function that when applied to a list, returns list \n",
    "# elements along with their indexes (counter); 1 indicates that the counter\n",
    "# should start from 1 instead of default 0\n",
    "for counter, sentence in enumerate(sentences, 1): \n",
    "    print(counter, sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'invested', 'in', 'six', 'areas', 'of', 'the', 'business', 'that', 'account', 'for', 'nearly', '40', '%', 'of', 'total', 'Macy', \"'s\", 'sales', '.', '\\n', 'Dresses', ',', 'fine', 'jewelry', ',', 'big', 'ticket', ',', 'men', \"'s\", 'tailored', ',', 'women', \"'s\", 'shoes', 'and', 'beauty', ',', 'these', 'investments', 'were', 'aimed', '\\n', 'at', 'driving', 'growth', 'through', 'great', 'products', ',', 'top', '-', 'performing', 'colleagues', ',', 'improved', 'environment', 'and', 'enhanced', '\\n', 'marketing', '.', 'All', 'six', 'areas', 'continued', 'to', 'outperform', 'the', 'balance', 'of', 'the', 'business', 'on', 'market', 'share', ',', 'return', 'on', '\\n', 'investment', 'and', 'profitability', '.', 'And', 'we', 'capture', 'approximately', '9', '%', 'of', 'the', 'market', 'in', 'these', 'categories', '.']\n",
      "1 We invested in six areas of the business that account for nearly 40% of total Macy's sales. \n",
      "\n",
      "\n",
      "2 Dresses, fine jewelry, big ticket, men's tailored, women's shoes and beauty, these investments were aimed \n",
      "at driving growth through great products, top-performing colleagues, improved environment and enhanced \n",
      "marketing.\n",
      "\n",
      "3 All six areas continued to outperform the balance of the business on market share, return on \n",
      "investment and profitability.\n",
      "\n",
      "4 And we capture approximately 9% of the market in these categories.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# load the English language model in spacy\n",
    "nlp = spacy.load('en_core_web_sm') \n",
    "# create an \"nlp\" object that parses a textual document\n",
    "a_text = nlp(text) \n",
    "\n",
    "# create a list of word tokens; note, this list will include punctuation marks and other symbols\n",
    "token_list = [] # start with an empty list\n",
    "for token in a_text:\n",
    "    token_list.append(token.text) # add a token to the token_list\n",
    "print(token_list) # print all identified tokens in text\n",
    "\n",
    "sentences = list(a_text.sents) # extract sentences\n",
    "\n",
    "# print all sentences\n",
    "for counter, sentence in enumerate(sentences, 1):\n",
    "    print(counter, sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming for 'increasing' is increas\n",
      "Stemming for 'increases' is increas\n",
      "Stemming for 'increased' is increas\n",
      "Lemmatization for 'increasing' is increase\n",
      "Lemmatization for 'increases' is increase\n",
      "Lemmatization for 'increased' is increase\n"
     ]
    }
   ],
   "source": [
    "# import Porter stemmer Module\n",
    "from nltk.stem import PorterStemmer \n",
    "# import WordNet lemmatization Module\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# object for Porter stemmer\n",
    "stemmer = PorterStemmer() \n",
    "# object for WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "# Then, performing stemming on single words is as simple as:\n",
    "print(f\"Stemming for 'increasing' is {stemmer.stem('increasing')}\")\n",
    "print(f\"Stemming for 'increases' is {stemmer.stem('increases')}\")\n",
    "print(f\"Stemming for 'increased' is {stemmer.stem('increased')}\")\n",
    "\n",
    "# To improve the accuracy of lemmatization, we need to provide each word's part of the speech (POS)\n",
    "print(f\"Lemmatization for 'increasing' is {lemmatizer.lemmatize('increasing', pos='v')}\") # specifying POS as verb \"v\"\n",
    "print(f\"Lemmatization for 'increases' is {lemmatizer.lemmatize('increases', pos='v')}\")\n",
    "print(f\"Lemmatization for 'increased' is {lemmatizer.lemmatize('increased', pos='v')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We deliv adjust earn per share of $ 2.12 . for the year , compar sale were down 0.7 % on an own plu licens basi , and we deliv adjust earn per share of $ 2.91 .\n",
      "We deliver adjusted earnings per share of $ 2.12 . For the year , comparable sale be down 0.7 % on an owned plus licensed basis , and we deliver adjusted earnings per share of $ 2.91 .\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet # WordNet is just another NLTK corpus reader\n",
    "# nltk.download('averaged_perceptron_tagger') # uncomment this line if 'averaged_perceptron_tagger' has not been yet downloaded\n",
    "from nltk import word_tokenize, pos_tag # import NLTK tokenizer and (part of speech) POS tagger\n",
    "from nltk.stem import PorterStemmer # import Porter stemmer class\n",
    "from nltk.stem import WordNetLemmatizer # import WordNet lemmatizer class\n",
    "from collections import defaultdict # default dictionary is similar to Python's regular dictionary, but allows the dictionary to return a default value if a requested key does not exist in the dictionary\n",
    "\n",
    "stemmer = PorterStemmer() # object for Porter stemmer\n",
    "lemmatizer = WordNetLemmatizer() # object for WordNet lemmatizer\n",
    "\n",
    "tag_map = defaultdict(lambda: wordnet.NOUN) # create a dictionary where single-letter keys are mapped to part of speech (noun, adjective, etc.) WordNet identifiers; by default, if a key does not exists the dictionary, return noun (wordnet.NOUN)\n",
    "tag_map['J'] = wordnet.ADJ # add key 'J' to the dictionary indicating adjective\n",
    "tag_map['V'] = wordnet.VERB # add key 'V' to the dictionary indicating verb\n",
    "tag_map['R'] = wordnet.ADV # add key 'R' to the dictionary indicating adverb\n",
    "\n",
    "text = \"We delivered adjusted earnings per share of $2.12. For the year, comparable sales were down 0.7% on an owned plus licensed basis, and we delivered adjusted earnings per share of $2.91.\"\n",
    "\n",
    "# function that stems text\n",
    "def stem_text(text:str):\n",
    "    tokens = word_tokenize(text) # split text into (word) tokens\n",
    "    stemmed_text = [] # start with an empty list\n",
    "    for token in tokens:\n",
    "        stem = stemmer.stem(token) # stem token\n",
    "        stemmed_text.append(stem) # append stemmed token to the stemmed_text list\n",
    "    return \" \".join(stemmed_text) # concatenate stemmed tokens elements with space (\" \") in-between\n",
    "\n",
    "# function that to lemmatizes text\n",
    "def lemmatize_text(text:str):\n",
    "    tokens = word_tokenize(text) # splits text into tokens\n",
    "    lemmatized_text = [] # start with an empty list\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        lemma = lemmatizer.lemmatize(token, tag_map[tag[0]]) # lemmatize word tokens, tag[0] returns POS letter identifier\n",
    "        lemmatized_text.append(lemma) # append lemmatized token to the lemmatized_text list\n",
    "    return \" \".join(lemmatized_text) # concatenate lemmatized tokens elements with space in-between\n",
    "\n",
    "# print stemmed version of text\n",
    "print(stem_text(text))\n",
    "# print lemmatized version of text\n",
    "print(lemmatize_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Dictionary-Based Word-Count Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[re.compile('\\\\bable\\\\b'), re.compile('\\\\babundance\\\\b'), re.compile('\\\\babundant\\\\b')]\n",
      "[re.compile('\\\\babandon\\\\b'), re.compile('\\\\babandoned\\\\b'), re.compile('\\\\babandoning\\\\b')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Let us start with a simple tone analysis, where each word is equally-weighted and we do not account for negators\n",
    "# First, we need to specify the locations of our dictionary files\n",
    "positive_words_dict = r\"./dictionaries/positive.txt\" # file path (location) to a text file with positive words; every word is in a separate line in the file\n",
    "negative_words_dict = r\"./dictionaries/negative.txt\" # file path to a text file with negative words\n",
    "\n",
    "# To be able to match all positive and negative words from the dictionaries, we need to create a list of regular expressions corresponding to these words\n",
    "# The following function reads all dictionary terms to a Python list, and converts the terms regular expressions\n",
    "def create_dict_regex_list(dict_file:str):\n",
    "    \"\"\"Creates a list of regex expressions of dictionary terms.\"\"\" # function description (optional)\n",
    "    with open(dict_file,\"r\") as file:  # opens the specified dict_file in \"r\" (read) mode \n",
    "        dict_terms = file.read().splitlines() # reads the content of the file line-by-line and creates a list of dictionary phrases\n",
    "    dict_terms_regex = [re.compile(r'\\b' + term + r'\\b') for term in dict_terms] \n",
    "    # re.compile(pattern) in Python compiles a regular expression pattern, which can be used for matching using its re.search, re.findall, etc.\n",
    "    # by adding \"\\b\" (i.e., word boundary) on each side of a dictionary term in Regex, we force an exact match that dictionary term\n",
    "    return dict_terms_regex # specifies the output of the function - in our case, a list of Regex expressions that correspond to the input dictionary file\n",
    "\n",
    "# Now we can apply our function to create Regex lists for positive and negative dictionary terms\n",
    "positive_dict_regex = create_dict_regex_list(positive_words_dict)\n",
    "negative_dict_regex = create_dict_regex_list(negative_words_dict)\n",
    "\n",
    "# print the first three entries of each Regex dictionary\n",
    "print(positive_dict_regex[0:3])\n",
    "print(negative_dict_regex[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(114, 7, 0, 6.140350877192983)\n"
     ]
    }
   ],
   "source": [
    "def get_tone (input_text:str):\n",
    "    \"\"\"Counts All and Specific Words in Text\"\"\" # function description (optional)\n",
    "    \n",
    "    ### Positive Words ###\n",
    "    \n",
    "    positive_words_matches = [re.findall(regex, input_text) for regex in positive_dict_regex] \n",
    "    # finds all regex matches and returns them as a list of lists\n",
    "    # so, the output of this search will be of the following format: [['able'], [], ['abundant','abundant'], [], ... ]\n",
    "    \n",
    "    positive_words_counts = [len(match) for match in positive_words_matches]\n",
    "    # len() measures the length of each list match\n",
    "    # so, the output of this list transformation will be of the following format: [1, 0, 2, 0, ...]\n",
    "    \n",
    "    positive_words_sum = sum(positive_words_counts) # sums all positive word counts in the counts list above\n",
    "    \n",
    "    ### Negative Words ###\n",
    "    \n",
    "    # in similar manner, we can get word counts for negative words\n",
    "    negative_words_matches = [re.findall(regex, input_text) for regex in negative_dict_regex] # finds all matches of negative words' regular expressions\n",
    "    negative_words_counts = [len(match) for match in negative_words_matches] # calculates the number of matches for each dictionary term regex\n",
    "    negative_words_sum = sum(negative_words_counts) # sums all negative word counts\n",
    "    \n",
    "    ### Total Words ###\n",
    "    total_words = re.findall(r\"\\b[a-zA-Z\\'\\-]+\\b\", input_text) # searches for all words in text, allowing apostrophes and hyphens in words, e.g., \"company's\", \"state-of-the-art\"\n",
    "    total_words_count = len(total_words) # calculates the number of all words in text\n",
    "    \n",
    "    # Finally, we can calculate Tone (expressed in % terms) as:\n",
    "    tone = 100 * (positive_words_sum - negative_words_sum)/total_words_count\n",
    "    return (total_words_count, positive_words_sum, negative_words_sum, tone)\n",
    "    \n",
    "# Applying our count_words function to an input text:\n",
    "counts = get_tone(\"At FedEx Ground, we have the market leading e-commerce portfolio. We continue to see strong demand across all customer segments with our new seven-day service. We will increase our speed advantage during the New Year. Our Sunday roll-out will speed up some lanes by one and two full transit days. This will increase our advantage significantly. And as you know, we are already faster by at least one day when compared to UPS's ground service in 25% of lanes. It is also really important to note our speed advantage and seven-day service is also very valuable for the premium B2B sectors, including healthcare and perishables shippers. Now, turning to Q2, I'm not pleased with our financial results.\")\n",
    "# output the results as (Total Word Count, Number of Positive Words, Number of Negative Words, Tone)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re.compile(\"(not|never|no|none|nobody|nothing|don\\\\'t|doesn\\\\'t|won\\\\'t|shan\\\\'t|didn\\\\'t|shouldn\\\\'t|wouldn\\\\'t|couldn\\\\'t|can\\\\'t|cannot|neither|nor)?\\\\s(able)\\\\b\")\n",
      "re.compile(\"(not|never|no|none|nobody|nothing|don\\\\'t|doesn\\\\'t|won\\\\'t|shan\\\\'t|didn\\\\'t|shouldn\\\\'t|wouldn\\\\'t|couldn\\\\'t|can\\\\'t|cannot|neither|nor)?\\\\s(abandon)\\\\b\")\n"
     ]
    }
   ],
   "source": [
    "# First, we update our function that compiles regular expressions\n",
    "def create_dict_regex_list_with_negators(dict_file:str):\n",
    "    \"\"\"Creates a list of regex expressions of dictionary terms.\"\"\"\n",
    "    with open(dict_file,\"r\") as file: \n",
    "        dict_terms = file.read().splitlines() # reads dictionary lines one-by-one\n",
    "    dict_terms_regex =[re.compile(r\"(not|never|no|none|nobody|nothing|don\\'t|doesn\\'t|won\\'t|shan\\'t|didn\\'t|shouldn\\'t|wouldn\\'t|couldn\\'t|can\\'t|cannot|neither|nor)?\\s(\" + term + r\")\\b\") for term in dict_terms] # the first capturing group in this Regex captures all possible negators, allowing for zero or one match as indicated by ? after the group; the second group captures dictionary terms\n",
    "    return dict_terms_regex # returns a list of Regex expressions that correspond to the input dictionary file, allowing for negators\n",
    "\n",
    "# Now we can apply our function to create Regex lists for positive and negative dictionary terms\n",
    "positive_dict_regex = create_dict_regex_list_with_negators(positive_words_dict)\n",
    "negative_dict_regex = create_dict_regex_list_with_negators(negative_words_dict)\n",
    "\n",
    "# prints the first entries of each Regex dictionary\n",
    "print(positive_dict_regex[0])\n",
    "print(negative_dict_regex[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('', 'advantage')\n",
      "('', 'advantage')\n",
      "('', 'advantage')\n",
      "('', 'leading')\n",
      "('not', 'pleased')\n",
      "('', 'strong')\n",
      "('', 'valuable')\n",
      "(114, 6, 0, 5.2631578947368425)\n"
     ]
    }
   ],
   "source": [
    "# calculates tone with negators\n",
    "def get_tone2 (input_text:str):\n",
    "    \"\"\"Counts All and Specific Words in Text, and checks for the presence of negators\"\"\" # function description (optional)\n",
    "    \n",
    "    total_words = re.findall(r\"\\b[a-zA-Z\\'\\-]+\\b\", input_text) # find all words in text\n",
    "    total_words_count = len(total_words) # calculate the number of all words\n",
    "    \n",
    "    # Positive Words #\n",
    "    # To account for negators, we can separately count positive and negated positive words\n",
    "    positive_word_count = 0 # initial values\n",
    "    negated_positive_word_count = 0 # initial values\n",
    "    \n",
    "    for regex in positive_dict_regex:\n",
    "        matches = re.findall(regex, input_text) # searches for all occurences of Regex\n",
    "        for match in matches:\n",
    "            if len(match)>0: # if match is not empty\n",
    "                print(match) # prints the match output; this is for illustration purposes (i.e., optional)\n",
    "            if match[0] == '': # if the first element of the match is empty, no negator is present\n",
    "                positive_word_count += 1 # so, increase the count of positive words by 1 \n",
    "            else:\n",
    "                negated_positive_word_count += 1 # otherwise, a negator is present, so increase the count of negated positive words by 1\n",
    "                \n",
    "   # If we are simply shifting the sentiment of negated positive words (from +1 to -1), then the final positive word count is just:\n",
    "    positive_words_sum = positive_word_count # i.e., count without negators\n",
    "    \n",
    "    # Repeat the same for Negative Words:\n",
    "    negative_word_count = 0 # initial values\n",
    "    negated_negative_word_count = 0 # initial values\n",
    "    \n",
    "    for regex in negative_dict_regex:\n",
    "        matches = re.findall(regex, input_text) # search for all occurences of Regex\n",
    "        for match in matches:\n",
    "            if len(match)>0: # if match is not empty\n",
    "                print(match) # check the match output         \n",
    "            if match[0] == '': # if the first element of the match is empty, no negator is present\n",
    "                negative_word_count += 1  # so, increase the count of negative words by 1\n",
    "            else:\n",
    "                negated_negative_word_count += 1 # otherwise, a negator is present, so increase the count of negated negative words by 1\n",
    "                \n",
    "   # If we are simply shifting the sentiment of negated negative words (from -1 to +1), then the final negative word count is just:\n",
    "    negative_words_sum = negative_word_count # i.e., count without negators\n",
    "    \n",
    "    # Then, Tone is:\n",
    "    tone = 100 * (positive_words_sum - negative_words_sum)/total_words_count\n",
    "    return (total_words_count, positive_words_sum, negative_words_sum, tone)\n",
    "\n",
    "# Applying function get_tone2 function to an example text:\n",
    "counts = get_tone2(\"At FedEx Ground, we have the market leading e-commerce portfolio. We continue to see strong demand across all customer segments with our new seven-day service. We will increase our speed advantage during the New Year. Our Sunday roll-out will speed up some lanes by one and two full transit days. This will increase our advantage significantly. And as you know, we are already faster by at least one day when compared to UPS's ground service in 25% of lanes. It is also really important to note our speed advantage and seven-day service is also very valuable for the premium B2B sectors, including healthcare and perishables shippers. Now, turning to Q2, I'm not pleased with our financial results.\")\n",
    "# output results\n",
    "print(counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

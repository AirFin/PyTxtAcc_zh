{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10: Sample Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all code from Chapter 10: _Measuring Text Similarity_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Text Similarity Measure for Long Text: Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.2 Representing text as a vector in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/roman/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/roman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#download NLTK's stopwords module\n",
    "nltk.download('stopwords')\n",
    "#downlod NLTK's punkt module\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~’\n"
     ]
    }
   ],
   "source": [
    "# Python includes a collection of all punctuation \n",
    "# characters\n",
    "from string import punctuation\n",
    "\n",
    "# add apostrophe to the punctuation character list\n",
    "punctuation_w_apostrophe = punctuation + \"’\"\n",
    "\n",
    "# print all characters\n",
    "print(punctuation_w_apostrophe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports word tokenizer from NLTK\n",
    "from nltk import word_tokenize\n",
    "# imports list of stop words from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "# imports Porter Stemmer module from NLTK\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# creates a list of English stop words\n",
    "set_stopwords = set(stopwords.words('english'))\n",
    "# creates a Porter stemmer object\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# creates a custom tokenizer that removes stop words, \n",
    "# punctuation, and stems the remaining words\n",
    "def custom_tokenizer(text:str):\n",
    "    # gets all tokens (words) from the lower-cased \n",
    "    # input text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # filters out stop words\n",
    "    no_sw_tokens = [t for t in tokens \n",
    "                    if t not in set_stopwords]\n",
    "    # filters out punctuation character tokens\n",
    "    no_sw_punct_tokens = [t for t in no_sw_tokens \n",
    "                          if t not in \n",
    "                          punctuation_w_apostrophe]\n",
    "    # stems the remaining words\n",
    "    stem_tokens = [stemmer.stem(t) for t in \n",
    "                   no_sw_punct_tokens]\n",
    "    # returns stemmed tokens (words)\n",
    "    return stem_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['verizon', 'commun', 'inc.', 'verizon', 'compani', 'hold', 'compani', 'act', 'subsidiari', 'one', 'world', 'lead', 'provid', 'commun', 'inform', 'entertain', 'product', 'servic', 'consum', 'busi', 'government', 'agenc']\n",
      "['lead', 'provid', 'commun', 'digit', 'entertain', 'servic', 'unit', 'state', 'world', 'offer', 'servic', 'product', 'consum', 'u.s.', 'mexico', 'latin', 'america', 'busi', 'provid', 'telecommun', 'servic', 'worldwid']\n",
      "['sprint', 'corpor', 'includ', 'consolid', 'subsidiari', 'commun', 'compani', 'offer', 'comprehens', 'rang', 'wireless', 'wirelin', 'commun', 'product', 'servic', 'design', 'meet', 'need', 'individu', 'consum', 'busi', 'govern', 'subscrib', 'resel']\n"
     ]
    }
   ],
   "source": [
    "# excerpt from Verizon Communications Inc. 2018 10-K\n",
    "doc_verizon = \"\"\"Verizon Communications Inc. (Verizon or the Company) is a holding company that, acting through its subsidiaries, is one of the world’s leading providers of communications, information and entertainment products and services to consumers, businesses and governmental agencies.\"\"\"\n",
    "# excerpt from AT&T Inc. 2018 10-K\n",
    "doc_att = \"\"\"We are a leading provider of communications and digital entertainment services in the United States and the world. We offer our services and products to consumers in the U.S., Mexico and Latin America and to businesses and other providers of telecommunications services worldwide.\"\"\"\n",
    "# excerpt from Sprint Corporation 2018 10-K\n",
    "doc_sprint = \"\"\"Sprint Corporation, including its consolidated subsidiaries, is a communications company offering a comprehensive range of wireless and wireline communications products and services that are designed to meet the needs of individual consumers, businesses, government subscribers and resellers.\"\"\"\n",
    "\n",
    "tokens_verizon = custom_tokenizer(doc_verizon)\n",
    "print(tokens_verizon)\n",
    "\n",
    "tokens_att = custom_tokenizer(doc_att)\n",
    "print(tokens_att)\n",
    "\n",
    "tokens_sprint= custom_tokenizer(doc_sprint)\n",
    "print(tokens_sprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['act', 'agenc', 'america', 'busi', 'commun', 'compani', 'comprehens', 'consolid', 'consum', 'corpor']\n",
      "[[1 1 0 1 2 2 0 0 1 0]\n",
      " [0 0 1 1 1 0 0 0 1 0]\n",
      " [0 0 0 1 2 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer converts text to bag-of-words vectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# creates a list of three documents; one for each \n",
    "# company\n",
    "documents = [doc_verizon,doc_att,doc_sprint]\n",
    "\n",
    "# creates a CountVectorizer object with the custom \n",
    "# tokenizer\n",
    "count_vectorizer = CountVectorizer(tokenizer=custom_tokenizer)\n",
    "\n",
    "# converts text documents to bag-of-word vectors\n",
    "count_vecs = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "# prints first ten bag-of-words features (words)\n",
    "print(count_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# prints first ten bag-of-words elements (counts) for \n",
    "# each vector the output is a matrix where each row \n",
    "# represents a document vector the element (count) \n",
    "# order in each vector corresponds to the order of \n",
    "# the bag-of-word features\n",
    "print(count_vecs.toarray()[:,:10]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.3 Calculating Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.44854261 0.40768712]\n",
      " [0.44854261 1.         0.32225169]\n",
      " [0.40768712 0.32225169 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# cosine_similarity calculates cosine similarity \n",
    "# between vectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# calculates text cosine similarity and stores results \n",
    "# in a matrix. The matrix stores pairwise similarity \n",
    "# scores for all documents, similarly to a covariance \n",
    "# matrix\n",
    "cosine_sim_matrix = cosine_similarity(count_vecs)\n",
    "\n",
    "# outputs the similarity matrix\n",
    "print(cosine_sim_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['act', 'agenc', 'america', 'busi']\n",
      "[[0.22943859 0.22943859 0.         0.13551013]\n",
      " [0.         0.         0.23464902 0.13858749]\n",
      " [0.         0.         0.         0.13365976]]\n"
     ]
    }
   ],
   "source": [
    "# TfidfVectorizer converts text to TF-IDF bag-of-words \n",
    "# vectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# creates a TfidfVectorizer object with the custom \n",
    "# tokenizer\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer)\n",
    "\n",
    "# converts text documents to TF-IDF vectors\n",
    "tfidf_vecs = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# prints first four bag-of-words features (words)\n",
    "print(tfidf_vectorizer.get_feature_names()[:4])\n",
    "\n",
    "# prints first four bag-of-words TF-IDF counts for each \n",
    "# vector. The output is a matrix where each row \n",
    "# represents a document vector\n",
    "print(tfidf_vecs.toarray()[:,:4]) # prints first four elements of each vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.30593809 0.23499515]\n",
      " [0.30593809 1.         0.17890296]\n",
      " [0.23499515 0.17890296 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# computes the cosine similarity matrix for TF-IDF \n",
    "# vectors\n",
    "tfidf_cosine_sim_matrix = cosine_similarity(tfidf_vecs)\n",
    "# outputs the similarity matrix\n",
    "print(tfidf_cosine_sim_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Text Similarity Measure for Short Text: Levenshtein Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.1 Introducing the Levenshtein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# edit_distance computes Levenshtein distance between \n",
    "# two pieces of text\n",
    "from nltk import edit_distance\n",
    "\n",
    "#example: account and accounts\n",
    "print(edit_distance(\"account\",\"accounts\"))\n",
    "\n",
    "#example: account and count\n",
    "print(edit_distance(\"account\",\"count\"))\n",
    "\n",
    "#example: account and access\n",
    "print(edit_distance(\"account\",\"access\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.2 Creating a Similarity Measure using the Levenshtein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity measure based on the Levenshtein distance\n",
    "# greater values indicate more similar text\n",
    "def edit_similarity(t1,t2):\n",
    "    # lowercase the input strings\n",
    "    (t1,t2) = (t1.lower(),t2.lower())\n",
    "    # calculates the Levenshtein distance between the \n",
    "    # input strings\n",
    "    distance = edit_distance(t1,t2)\n",
    "    # calculates length of the longest input string\n",
    "    longest_text_len = max(len(t1),len(t2))\n",
    "    # if both t1 and t2 are empty strings, they are \n",
    "    # identical; thus return 1 as the output\n",
    "    if longest_text_len == 0:\n",
    "        return 1.0\n",
    "    # else compute the similarity measure as \n",
    "    # 1 - (levenshtein_distance / length of the longest input string)\n",
    "    else:\n",
    "        return (1.0 - float(distance) / float(longest_text_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein distance: 11\n",
      "Levenshtein similarity score: 0.7105263157894737\n"
     ]
    }
   ],
   "source": [
    "# original company name\n",
    "orig_name = \"Fidelity National Information Services\"\n",
    "# shortened company name\n",
    "comp_name = \"Fidelity National Info Svcs\"\n",
    "\n",
    "# calculates and outputs the Levenshtein distance\n",
    "levenshtein_distance = edit_distance(orig_name,comp_name)\n",
    "print(\"Levenshtein distance:\",levenshtein_distance)\n",
    "\n",
    "# calculates and output the similarity score based on \n",
    "# Levenshtein distance\n",
    "levenshtein_similarity = edit_similarity(orig_name,comp_name)\n",
    "print(\"Levenshtein similarity score:\",levenshtein_similarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
